\documentclass[]{article}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{xcolor}

% Use commented out command when done
%\newcommand{\comment}[1]{}
\newcommand{\comment}[1]
{\par {\bfseries \color{green} #1 \par}}

%opening
\title{Haiku Generations}
\author{Zachary A Bookey \and Max Magnuson}

\begin{document}

\maketitle

\begin{abstract}
Something something I'm really bad at abstracts...
\end{abstract}

\section{Introduction}
Give a quick overview of what we did for our project.

\section{Background}

\subsection{Haiku}
Haikus are three line poems with the second line longer than the other two. In english haikus the lines are measured by the number of syllables. The most common configuration is 5-7-5 in which the first and second lines of the poem have five syllables, and the second line of the poem has seven syllables. Figure \ref{fig:WrightHaiku} uses this structure. Some haikus contain fewer than seventeen syllables, but rarely exceed that amount \cite{Higginson}. \comment{"Need to explain syllables?"}

\begin{figure}[H]
	\centering
	From the skyscraper, \break
	All the bustling streets converge \break
	Towards a spring sea.
	\caption{Haiku composed by Richard Wright \cite{Terebess}}
	\label{fig:WrightHaiku}
\end{figure}

Due to the constraints of haikus, authors need to be careful in choosing each word. With so few syllables to work with, each word contributes significantly more to the overall meaning of the poem than other poetic forms. Therefore, haikus need to be terse and not use complete sentences. In figure \ref{fig:WrightHaiku} only the second line can be considered a complete sentence.

\subsection{Markov Chains}

Markov chains are a collection of states or variables with an associated collection of probabilities for each state that denotes the chance of going from one state to another. Markov chains can be represented as graphs. In figure \ref{fig:Chain} A, B, and C are nodes that represent the collection of states. The edges represent probabilities. For example, the likeliness of going from state A to state B is 0.25. Note that the edges of each node sum to 1. This is important to maintain because the nonzero edges connect to all possible states that can be achieved from a specific node. This means that in figure \ref{fig:Chain}, if the current state is B, then the possible states that follow are A and C. B cannot follow another B since the probability to itself is zero. \cite{Markov}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{MarkovChainExample}
	\caption{Example Markov Chain}
	\label{fig:Chain}
\end{figure}

<<<<<<< HEAD
\section{Haiku Constraint Problem}
Demonstrate how a haiku can be represented as a constraint problem. For example explain how each word can be represented as a node and a link of nodes with the right number of syllables is a line and three lines makes a haiku.
\comment{TODO: Finish section}
=======
\section{Haiku Constraints}
With a dictionary of words to choose from, the haiku generation problem is easily represented as a constraint problem due to its rigid structure. Since each haiku is composed of three lines with a fixed syllable count the problem can be broken down by representing each line as its own constraint problem. The task then becomes to fill in each line with words that satisfy the total number of syllables required for each line.
>>>>>>> 15b282013e20e9cfa15e880fb9b4b063f529521a

\subsection{Counting Syllables}
\comment{TODO: Finish section, replaces part in next section about syllables}

\subsection{The Search Space}
Our search space was represented by a Markov chain where each variable is a word and relationships between words are determined by the likelihood that one word is succeeded or proceeded by another. However, in a Haiku we need to know more than just what the word is, we also need to know the sounds it makes, most notably the number of syllables it contains. Unfortunately, neither of us are experts in the English language and after some looking it appears that counting syllables is not an exact science and there are always exceptions to the rules. We implemented a method that accounts for basic English patterns and counts syllables of each word based on these, thus it's not perfect but adequate enough for our work.

To generate these relationships and ultimately the Markov chain, we require that the user provide a text file filled with sentences and phrases that get read in and the relationships between words in that file become the basis for our Markov chain. We treat each line in the input file independently from the previous, meaning that the last word of the first line will not have a relationship with the first word in the second. The parser starts each line by reading the first word and counting its syllables and adding it to the chain the second word is then read in and a connection from the first word to the second word is created and a counter of how many times the relationship appears is incremented. This process then repeats until the entire file has been read and converted to the Markov chain. At this point, instead of probabilities the relations between words has a count of how many times this relationship appears, so for each node in the chain we need to convert these into probabilities by summing the total number of relationships and dividing each count by this sum. The end result of parsing the file is a Markov chain that contains a node for every word in the file and a relationship for ever sequence of words that appears in the file.
\comment{Should an example markov chain and input file be showcased here to demonstrate how this works and lead into the next paragraph?}
Since space is at a premium in a haiku it may not be worth adding short and meaningless words such as prepositions. To accommodate for that a modifier flag has been built into the parser to ignore a list of user defined words. For example if a user wanted to ignore the word ``a'' the phrase ``eat a vegetable'' would omit ``a'' from the Markov chain and create a relationship instead between the words ``eat'' and ``vegetable.''

\section{Search Algorithms}
With the search space created the final step in generating a haiku was determining a way to navigate the search space We considered the trivial approach of choosing words at random from the Markov chain and adding them to the haiku where ever they may fit but odds are this would result in a lackluster haiku. Instead we decided to take three different approaches to navigating the space. The first being a slight spin on the trivial approach in that we choose our next word for the haiku from the previous word if possible. The second was a depth first approach in a way that would increase fluidity between words and hopefully generate a haiku that made sense. Finally we implemented a 

\subsection{Naive Search}
The first search method we attempted in our search space was trivial and consisted of picking a starting node in the chain and adding that word to the first available spot in any line of the haiku. The method then chose the next word from the chain as a child of the previous word and added it to any available space in the haiku that would not violate a syllable constraint. If the word could not be added it was ignored and used to generate the next word from it's children. If this approach ever reached a dead end in the chain it would pick a random node from the chain and continue from where it left off.\comment{"Give a one sentence intro about short random walks"}

\begin{algorithm}[H]
	\caption{$Naive\_Search()$} \label{Naive}
	\begin{algorithmic}[1]
		\State Choose a random start node and set it to $current$
		\While{Haiku is not full}
			\State if possible, add $current$ to open line in haiku
			\If{$current$ has children}
				\State choose random $child$ from $current$
				\State $current = child$
			\Else
				\State Choose a random node and set it to $current$
			\EndIf
		\EndWhile
	\end{algorithmic}
\end{algorithm}

While this method works for generating valid Haiku's it's rarely any better than throwing random words into a haiku and hoping there is some meaning to the madness. To increase the likelihood that a haiku generated has a connection between each word in the line it is best to force words that are contiguous in the haiku to be connected in the markov chain. We implemented depth first search and an iterative random walk method to accomplish this.

\subsection{Depth First Search}
One of the methods we used to navigate the search space and generate a Haiku was depth first search with restarts. To accomplish this we utilized a two-way linked list where each node contained a markov node and generated it's child randomly by weighting each of the markov nodes children's probability to generate each line individually. To initialize the search we chose a random markov node from our markov chain and had our root list node point to this node. The algorithm then chose a child node from that markov node and determined how many syllables the line of words would currently have and if we're still within the constraint it would repeat. If we have exactly the number of syllables we needed the code would return the line represented by this linked list starting from the root and ending at the child. If the child we chose would violate our constraints we retreated to the parent and would randomly choose a child that we had not yet visited from this node. If no children we're available we would retreat up another node. Finally if we are at the root node and there are no children available the code would restart and choose a new random starting position in the markov chain.

\begin{algorithm}[H]
	\caption{$Depth\_First\_Search(n)$} \label{DFSB}
	\begin{algorithmic}[1]
		\State Choose an random $root$ node from the chain.
		\State $current = root$
		\While {$Syllables \neq n$}
			\If{Have not visited all possible children of $current$}
				\State Choose random unvisited $child$ of $current$
				\If{$child.syllables \leq n$}
					\State $current = child$
				\EndIf
			\Else
				\If{$current == root$}
					\Return $Depth\_First\_Search(n)$
				\Else
					\State $current$ = $parent$ of $current$
				\EndIf
			\EndIf
		\EndWhile
		\Return Line represented by linked list defined by $root$
	\end{algorithmic}
\end{algorithm}

This approach was then extended to try and maintain fluidity between lines. Instead of generating a random node for the starting position of the line we attempted to use a child of the last node from the previous line. If the child we chose was unable to produce a valid line for the haiku we chose a different child of the previous line's last node. If we had exhausted all the children then the algorithm opted to lose fluidity and generate a random starting node for this line.

\begin{algorithm}[H]
	\caption{$Depth\_First\_Search(node, n)$} \label{DFSB_WithStart}
	\begin{algorithmic}[1]
		\State $root = node$
		\State $current = root$
		\While {$Syllables \neq n$}
		\If{Have not visited all possible children of $current$}
		\State Choose random unvisited $child$ of $current$
		\If{$child.syllables \leq n$}
		\State $current = child$
		\EndIf
		\Else
		\If{$current == root$}
			\State {generate new $child$ from the $parent$ of $root$}
			\Return {$Depth\_First\_Search(child, n)$}
		\Else
		\State $current$ = $parent$ of $current$
		\EndIf
		\EndIf
		\EndWhile
		\Return Line represented by linked list defined by $root$
	\end{algorithmic}
\end{algorithm}

A final extension of this approach that is currently being implemented is to choose a starting node for the whole haiku and generate the entire haiku using just that node. Some haiku's generated by the previous approach would be valid in this sense but since any line could be restarted with random node not all haiku's would be fluid throughout. This approach would need to be able to regenerate the previous line if the children of the last node cannot generate a new line.

% Create pseudo code for this approach...

\subsection{Iterative Random Walk}

Iterative random walk leverages the naive approach in an iterative fashion to improve the quality of the generated haikus. The algorithm iteratively generates five and seven syllable lines starting from the same node in the Markov chain while noting the number of nodes visited. If the algorithm chooses lines that used the fewest number of nodes, then the line should have the closest thematic meaning to the root node. This is due to the assumption that nodes with nonzero probabilities are related thematically.

\begin{algorithm}[H]
	\caption{$IterativeRandomWalk(n)$} \label{IterativeRandomWalk}
	\begin{algorithmic}[1]
		\State Choose random node to be $root$
		\State $fiveLines$
		\State $sevenLines$
		\For 0 to $n$-1
			\State $line$, $steps$ = $RandomWalk(root, 5)$
			\State add $line$, $steps$ to $fiveLines$
			\State $line$, $steps$ = $RandomWalk(root, 7)$
			\State add $line$, $steps$ to $sevenLines$ 
		\EndFor
		\State sort $fiveLines$ by $steps$
		\State sort $sevenLines$ by $steps$
		\State $Haiku$
		\State add first two lines of $fiveLines$ to $Haiku$
		\State add first line of $sevenLines$ to $Haiku$
		\State return $Haiku$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{$RandomWalk(root, j)$} \label{RandomWalk}
	\begin{algorithmic}[1]
		\State $steps$ = 0
		\State $syllables$ = 0
		\State $line$
		\State $current$ = $root$
		\While $syllables$ < $j$
			\If{$current.Syllables$ + $syllables$ <= $j$}
				$syllables$ = $syllables$ + $current.Syllables$
				\State add $current.word$ to $line$
			\EndIf
			\State choose random $child$ from $current$
			$current$ = $child$
			$steps$ = $steps$ + 1
		\EndWhile
		\State return $line$, $steps$
	\end{algorithmic}
\end{algorithm}

Iterative random walk first chooses a random node from the Markov chain to be the root node. During each iteration the algorithm performs a random walk through the Markov chain starting from the root node. At each step of the random walk, the word from the current node is added to a collection if adding that word does not exceed the maximum number of syllables. Once the maximum number of syllables is reached, the collection is then returned along with the number of nodes visited. Once all of the iterations have been completed, the algorithm chooses the two five syllable lines with the fewest nodes visited and the seven syllable line with the fewest nodes visited to construct a haiku. 

\comment{Should I explain random walk? May as well. No harm in being overdescriptive.}

\section{Results}
Give some examples of the haiku's we were able to identify and acknowledge some of the highlights and short comings of the work we did.

Round One: Aesop Lyrics

\begin{figure}[H]
	\centering
	dry grays supplied raise \break
	fiery colossus dead tombs \break
	vibes blissful light
	\caption{Naive approach from Aesop Rock lyrics}
	\label{fig:NaiveAesop}
\end{figure}

\begin{figure}[H]
	\centering
	thieves received \break
	blue face through terrors of woe \break
	brain tripped beta
	\caption{DFS approach one from Aesop Rock lyrics}
	\label{fig:DFSOneAesop}
\end{figure}

\begin{figure}[H]
	\centering
	recording til fell \break
	through mite infested grillage \break
	awaited my game
	\caption{DFS approach two from Aesop Rock lyrics}
	\label{fig:DFSTwoAesop}
\end{figure}

\begin{figure}[H]
	\centering
	clash to conflicting \break
	clash to insanity dolls \break
	clash to emerge
	\caption{Iterative Random Walk from Aesop Rock lyrics}
		\label{fig:IRWAesop}
	\end{figure}

%%Round Two: Shakespeare

\begin{figure}[H]
	\centering
	leprosy i' faith \break
	o'ertake torn owner's \break
	tongue was sleeping
	\caption{Naive from Shakespeare}
	\label{fig:NaiveShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	cites virtuous \break
	admiring praise that i have \break
	driven on his name
	\caption{DFS approach one from Shakespeare}
	\label{fig:DPSOneShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	horror that i saw \break
	her audit though my uncle \break
	forest looks bleak air
	\caption{DFS approach two from Shakespeare}
	\label{fig:DFSTwoShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	slipper'd pantaloon \break
	penury these things without \break
	slipper'd pantaloon
	\caption{Iterative random walk from Shakespeare}
	\label{fig:IRWShakespeare}
\end{figure}

%%Round Three: Aesop + Shakespeare

\begin{figure}[H]
	\centering
	lendin' bended dream \break
	be your praise confound thou \break
	distinction know'st
	\caption{Naive from Aesop Rock lyrics and Shakespeare}
	\label{fig:NaiveAesopShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	such contempt of truth \break
	disobedience he has \break
	demurely wake
	\caption{DFS approach one from Aesop Rock lyrics and Shakespeare}
	\label{fig:DFSOneAesopShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	over-goes my son \break
	was his mouth that i know more \break
	bright than i do look
	\caption{DFS approach two from Aesop Rock lyrics and Shakespeare}
	\label{fig:DFSTwoAesopShakespeare}
\end{figure}

\begin{figure}[H]
	\centering
	mood smell onions \break
	mood smell onions i say \break
	mood ring militant
	\caption{Iterative random walk from Aesop Rock lyrics and Shakespeare}
	\label{fig:IRWAesopShakespeare}
\end{figure}

\section{Conclusion}
Conclude yo.

\section{Related Work}
List of related works?
\comment{Don't think we need this section}

\bibliography{mybib}
\bibliographystyle{plain}
\end{document}
